{"cells":[{"cell_type":"markdown","metadata":{"id":"GQWoTymmtnW8"},"source":["# CSE527 Homework 5 Part 1\n","**Due date: 11:59 pm EST on May 9, 2022 (Monday)**\n","\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","---\n","\n","## Tracking algorithm description\n","\n","This assignment is based on SiamFC [1] Tracking. The basic idea of [1] is to learn a similarity measurement between target and search space using deep Siamese network. With the learned similarity measurement, both target localization and target scale estimation will be performed on it. In addition, no model update is performed during tracking. **More details can be found in the original paper [1].**\n","\n","## Your task\n","\n","In this assignment, you will find two problems.\n","1. The first one is a standard SiamFC based tracking algorithm. It will be given with several parts incompleted (e.g., the function loading sequence for tracking). You need to complete these parts and make the full tracking algorithm work. \n","2. Second one is based on RGB-Thermal Tracking. In this tracking problem, you will additionally have a Thermal channel alongside the RGB(visible spectrum) image in the video sequences. You need to use this additional information to your advantage to aid the tracking. Again, few missing sections of the code have to be completed here.   \n","\n","\n","The incompleted parts will be like this:\n","\n","  \\# description of requirement for this part\n","\n","  \\##########--WRITE YOUR CODE HERE--##########\n","\n","  ...\n","\n","  \\##########-------END OF CODE-------##########\n","\n","**NOTE: the pretrained SiamFC model is given, and you just need to use the pretrained model to finish the tracking algorithm.**\n","\n","\n","[1] Fully-Convolutional Siamese Networks for Object Tracking, ECCVW, 2016.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tbTKqoSyJOzv"},"source":["#Environment set up and data preparation\n","\n","\n","\n","1.   **Install PyTorch 1.1.0 and Pillow 6.0.0.**\n","\n","  Please install PyTorch 1.1.0 using the following command (By default, latest version of PyTorch is installed on CoLab. Here, we want to use an old version). In addition, you also need to downgrade the Pillow to verion 6.0.0. \n","  \n","  **After finishing, remember to restart runtime, otherwise you may get errors.**\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zfDf68-8u7q3","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/","height":894},"executionInfo":{"status":"ok","timestamp":1652281728052,"user_tz":240,"elapsed":61958,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"outputId":"cc327558-f2cb-4720-e78a-650b14acc3f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch===1.1.0\n","  Downloading torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9 MB)\n","\u001b[K     |████████████████████████████████| 676.9 MB 4.1 kB/s \n","\u001b[?25hCollecting torchvision==0.4.0\n","  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.4.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch===1.1.0) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0) (1.15.0)\n","  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.4.0%2Bcpu-cp37-cp37m-manylinux1_x86_64.whl (12.2 MB)\n","\u001b[K     |████████████████████████████████| 12.2 MB 6.0 MB/s \n","\u001b[?25h  Downloading torchvision-0.4.0-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 32.9 MB/s \n","\u001b[?25hINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","\u001b[31mERROR: Cannot install torch===1.1.0, torchvision==0.4.0, torchvision==0.4.0+cpu and torchvision==0.4.0+cu92 because these package versions have conflicting dependencies.\u001b[0m\n","\n","The conflict is caused by:\n","    The user requested torch===1.1.0\n","    torchvision 0.4.0+cu92 depends on torch==1.2.0\n","    The user requested torch===1.1.0\n","    torchvision 0.4.0+cpu depends on torch==1.2.0\n","    The user requested torch===1.1.0\n","    torchvision 0.4.0 depends on torch==1.2.0\n","\n","To fix this you could try to:\n","1. loosen the range of package versions you've specified\n","2. remove package versions to allow pip attempt to solve the dependency conflict\n","\n","\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n","Collecting pillow==6.0.0\n","  Downloading Pillow-6.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 5.4 MB/s \n","\u001b[?25hInstalling collected packages: pillow\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.0.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed pillow-6.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}}],"source":["# install pytorch\n","!pip install torch===1.1.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# downgrade pillow\n","!pip install pillow==6.0.0\n","\n","# after executing the above commands, remember to restart runtime"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9hN9Cku0mI4X","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652281765657,"user_tz":240,"elapsed":19800,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"outputId":"94474c62-00c5-45f4-bf8e-93febd29a9f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/MyDrive/Halwah_Amro_111073151_HW5\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd \"/gdrive/MyDrive/Halwah_Amro_111073151_HW5\""]},{"cell_type":"markdown","metadata":{"id":"IvFjab6HwBFa"},"source":["2.   **Download data and model**\n","\n","Now lets pull the data into your project root.  \n","If you are using Google colab, you can simply open each below link and click the \"Add to Drive\" Icon on the top-right corner of website to add it's shortcut to your current project root.  \n","\n","link for data:\n","  1.  https://drive.google.com/open?id=1GH1mDaS6vWXcypbXxqjVxtcQM0XY28lS  \n","  2. https://drive.google.com/open?id=10lIlhq15W0EuspAk-s-u5JNei1UyEJ-s  \n","\n","link for model:  \n","  3. https://drive.google.com/open?id=1XhmZWFxth4JDbf_Pd_qezEVRu-D_UPZ8;\n","\n","link for utility codes: \n","  4. https://drive.google.com/open?id=1lUh8GO-wX0rxaxJGLefpute7_YK1Sz7H \n","  5. https://drive.google.com/open?id=1A_KbyWcOpl-6cEW7zP0Ub-fE1dVjZSgE;\n","\n","\n","Else, you can manually download the above files to your local project root."]},{"cell_type":"markdown","metadata":{"id":"OxacLiMLM1Xw"},"source":["3. **Unzip the video sequence using the following commands.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueSNAoOZ238k","vscode":{"languageId":"python"}},"outputs":[],"source":["# unzip video data from the zip file\n","!unzip SiamFCVideo.zip\n","!unzip RGB-T234.zip\n","# already unzipped"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akKVq-gH3Nw4","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651596023745,"user_tz":240,"elapsed":233,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"outputId":"a43c1378-d499-4d44-968d-55c248713122"},"outputs":[{"output_type":"stream","name":"stdout","text":["Config.py\t\t faster-rcnn.pytorch  SiamFCMModel.pth\tSiamNet.py\n","CSE527-22S-HW5-P1.ipynb  RGB-T234\t      SiamFCVideo\n","CSE527-22S-HW5-P2.ipynb  RGB-T234.zip\t      SiamFCVideo.zip\n"]}],"source":["# check if successfully uploading the files\n","# should show: Config.py, SiamFCModel.pth, SiamFCVideo, SiamFCVideo.zip, RGB-T234, RGB-T234.zip, SiamNet.py\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"u8wpRSAKLZe4"},"source":["# Problem 1: SiamFC Tracking Algorithm\n","\n","(35 points) **Some important instructions:**\n","\n","\n","*   After finishing preparing the data, it is better to check each of them;\n","*   The network of SiamFC is defined in *SiamNet.py*, and you can check it to get familar with the architecture of SiamFC;\n","*   There are some parameters defined in the Config.py, and you need to know some of them for coordinate transformation;\n","\n","\n","**NOTE 1: before starting the implementation try to read what existing methods are trying to acheive, so you know how to use them.**\n","\n","**NOTE 2: before running the code, please change runtime type to GPU**\n","\n","**NOTE 3: There are many available resources about SiamFC online, and you should finish this assignment on your own. Do NOT copy-and-paste!!!**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kgVMaGc_3iNe","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1652281832341,"user_tz":240,"elapsed":58679,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Tso0Z4karT9wWMKl6Meowb74CxLSu72u"},"outputId":"53ef58c8-525c-46bc-f65b-9b32de1db260"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Code for SiamFC tracking algorithm\n","import torchvision.transforms.functional as F\n","import cv2\n","from torch.autograd import Variable\n","import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import glob\n","from Config import *\n","from IPython.display import clear_output\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def load_sequence(seq_path):\n","    \"\"\"\n","    load sequences;\n","    sequences should be in OTB format, or you can custom this function by yourself\n","    \"\"\"\n","    # given the sequence path, you need to load sequence information, including\n","    # img_list: a list, each element of which is the path for an image\n","    # target_position: an array, the center position of the target [center_y, center_x]\n","    # target_sz: an array, the height and width of the target [h, w]\n","    # NOTE: the sequence folder contains a sub-folder that contains all image files, \n","    # and a 'groundtruth_rect.txt' that contains the initial groundtruth of the target in format (top_left_x, top_left_y, width, height)\n","\n","    ##########--WRITE YOUR CODE HERE--##########\n","    imgpath = os.path.join(seq_path,'img')\n","    groundtruthfile = os.path.join(seq_path, 'groundtruth_rect.txt')\n","    img_list = []\n","\n","    img_list = [imgfile for imgfile in glob.glob(os.path.join(imgpath, \"*.jpg\"))]\n","\n","    img_list.sort()\n","    \n","    groundtruth = open(groundtruthfile, 'r')\n","    reader = groundtruth.readline()\n","    region = [float(i) for i in reader.strip().split(\",\")]\n","    top_left_x, top_left_y = region[0], region[1]\n","    w, h = region[2], region[3]\n","    target_sz = np.array([h,w], dtype = np.double)\n","    cx = top_left_x + w/2\n","    cy = top_left_y + h/2\n","    target_position= np.array([cy, cx], dtype = np.double)\n","    ##########-------END OF CODE-------########## \n","\n","    return img_list, target_position, target_sz\n","\n","\n","def visualize_tracking_result(img, bbox, fig_n):\n","    \"\"\"\n","    visualize tracking result\n","    \"\"\"\n","    fig = plt.figure(fig_n)\n","    ax = plt.Axes(fig, [0., 0., 1., 1.])\n","    ax.set_axis_off()\n","    fig.add_axes(ax)\n","    r = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth = 3, edgecolor = \"#00ff00\", zorder = 1, fill = False)\n","    ax.imshow(img)\n","    ax.add_patch(r)\n","    plt.ion()\n","    plt.show()\n","    plt.pause(0.00001)\n","    plt.clf()\n","\n","\n","def get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans):\n","    \"\"\"\n","    extract image crop \n","    \"\"\"\n","    # im: input image\n","    # pos: [center_y, center_x] of the position around which image has to be cropped\n","    # model_sz: size to which the cropped portion has to be rescaled before return\n","    # original_sz: size of the patch to be cropped from the original image\n","    # avg_channels: a 3 sized array indicating avg intensities of pixels in RGB. This\n","    # color is used for padding areas \n","\n","    if original_sz is None:\n","        original_sz = model_sz\n","\n","    sz = original_sz\n","    im_sz = im.shape\n","    # make sure the size is not too small\n","    assert (im_sz[0] > 2) & (im_sz[1] > 2), \"The size of image is too small!\"\n","    c = (sz+1) / 2\n","\n","    # check out-of-bounds coordinates, and set them to black\n","    context_xmin = round(pos[1] - c)       # floor(pos(2) - sz(2) / 2);\n","    context_xmax = context_xmin + sz - 1\n","    context_ymin = round(pos[0] - c)       # floor(pos(1) - sz(1) / 2);\n","    context_ymax = context_ymin + sz - 1\n","\n","    # compute padding\n","    left_pad = max(0, 1-context_xmin)       # in python, index starts from 0\n","    top_pad = max(0, 1-context_ymin)\n","    right_pad = max(0, context_xmax - im_sz[1])\n","    bottom_pad = max(0, context_ymax - im_sz[0])\n","\n","    context_xmin = context_xmin + left_pad\n","    context_xmax = context_xmax + left_pad\n","    context_ymin = context_ymin + top_pad\n","    context_ymax = context_ymax + top_pad\n","\n","    im_R = im[:, :, 0]\n","    im_G = im[:, :, 1]\n","    im_B = im[:, :, 2]\n","\n","    # padding\n","    if (top_pad !=0) | (bottom_pad !=0) | (left_pad !=0) | (right_pad !=0):\n","        im_R = np.pad(im_R, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), 'constant', constant_values = avg_chans[0])\n","        im_G = np.pad(im_G, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), 'constant', constant_values = avg_chans[1])\n","        im_B = np.pad(im_B, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), 'constant', constant_values = avg_chans[2])\n","\n","        im = np.concatenate([im_R[...,np.newaxis], im_G[...,np.newaxis], im_B[...,np.newaxis]], axis=2, dtype=np.double)\n","\n","    im_patch_original = im[int(context_ymin)-1:int(context_ymax), int(context_xmin)-1:int(context_xmax), :]\n","\n","    # resize to the 'model_sz'\n","    if model_sz != original_sz:\n","        im_patch = cv2.resize(im_patch_original, (int(model_sz), int(model_sz)), interpolation = cv2.INTER_CUBIC)\n","    else:\n","        im_patch = im_patch_original\n","\n","    return im_patch\n","\n","\n","def make_scale_pyramid(im, target_position, in_side_scaled, out_side, avg_chans, p):\n","    \"\"\"\n","    extract multi-scale image crops\n","    Used for extracting different scales of area of interest from the search image\n","    \"\"\"\n","    # im: image\n","    # target_position: center of the area of interest\n","    # in_side_scaled: The lengths of the sides of 'area of interest' squares at differnt scales   \n","    # out_side: The resolution (of a side) of each pyramid level \n","    # avg_chans: a 3 sized array indicating avg intensities of pixels in RGB. This\n","    # color is used for padding areas \n","    # p: config\n","\n","    in_side_scaled = np.round(in_side_scaled)\n","    pyramid = np.zeros((out_side, out_side, 3, p.num_scale), dtype = np.double)\n","    max_target_side = in_side_scaled[in_side_scaled.size-1]\n","    min_target_side = in_side_scaled[0]\n","    beta = out_side / min_target_side\n","    # size_in_search_area = beta * size_in_image\n","    # e.g. out_side = beta * min_target_side\n","    search_side = round(beta * max_target_side)\n","\n","    search_region = get_subwindow_tracking(im, target_position, search_side, max_target_side, avg_chans)\n","\n","    assert (round(beta * min_target_side) == out_side), \"Error!\"\n","\n","    # extract multiple pyramid patches using get_subwindow_tracking() function;\n","    # you should use a loop to do this;\n","    # the number of scales is indicated by p.num_scale;\n","    # the scale information is stored in in_side_scaled[];\n","    # the obtained pryamid is represented in variable 'pyramid';\n","    \n","    ##########--WRITE YOUR CODE HERE--##########\n","    for n in range(p.num_scale):\n","      pyramid[:,:,:,n] = get_subwindow_tracking(search_region, [1+search_side/2, 1+search_side/2], out_side, round(beta * in_side_scaled[n]), avg_chans)\n","\n","    ##########-------END OF CODE-------##########  \n","    return pyramid\n","\n","\n","def tracker_eval(net, s_x, z_features, x_features, target_position, window, p):\n","    \"\"\"\n","    do evaluation (i.e., a forward pass for search region)\n","    \n","    This method takes in the feature maps of search and the target image and finds\n","    the target position and scale. \n","    \"\"\"\n","    # net: model\n","    # s_x: size of the area of interest patch cropped from original input search image\n","    # z_features: siamFC features of target image\n","    # x_features: siamFC features of search image\n","    # target_position: center of the area of interest from original input image\n","    # window: a 2D array to adapt the response map for better tracking\n","    # p: config\n","    \n","    # compute scores search regions of different scales\n","    scores = net.xcorr(z_features, x_features)\n","    scores = scores.to(\"cpu\")\n","\n","    response_maps = scores.squeeze().permute(1, 2, 0).data.numpy()\n","    # for this one, the opencv resize function works fine\n","    response_maps_up = cv2.resize(response_maps, (response_maps.shape[0]*p.response_UP, response_maps.shape[0]*p.response_UP), interpolation=cv2.INTER_CUBIC)\n","\n","    # choose the scale whose response map has the highest peak\n","    if p.num_scale > 1:\n","        current_scale_id =np.ceil(p.num_scale/2)\n","        best_scale = current_scale_id\n","        best_peak = float(\"-inf\")\n","        for s in range(p.num_scale):\n","            this_response = response_maps_up[:, :, s]\n","            # penalize change of scale\n","            if s != current_scale_id:\n","                this_response = this_response * p.scale_penalty\n","            this_peak = np.max(this_response)\n","            if this_peak > best_peak:\n","                best_peak = this_peak\n","                best_scale = s\n","        response_map = response_maps_up[:, :, int(best_scale)]\n","    else:\n","        response_map = response_maps_up\n","        best_scale = 1\n","    # make the response map sum to 1\n","    response_map = response_map - np.min(response_map)\n","    response_map = response_map / sum(sum(response_map))\n","\n","    # apply windowing\n","    response_map = (1 - p.w_influence) * response_map + p.w_influence * window\n","    p_corr = np.asarray(np.unravel_index(np.argmax(response_map), np.shape(response_map)))\n","\n","    # avoid empty\n","    if p_corr[0] is None:\n","        p_corr[0] = np.ceil(p.score_size/2)\n","    if p_corr[1] is None:\n","        p_corr[1] = np.ceil(p.score_size/2)\n","\n","    # Convert to crop-relative coordinates p_corr to frame coordinates\n","    # the frame coordinates are represented using new_target_position (it is an array)\n","    # Hint: You may do this part by computing:\n","    # 1. displacement from the center in instance final representation\n","    # 2. displacement in instance input\n","    # 3. displacement in instance original crop (in frame coordinates)\n","    # 4. new target position within frame in frame coordinates\n","    # (You need to use some parameters in Config for this part)\n","\n","\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","\n","\n","    distance = p_corr - np.ceil(p.score_size * p.response_UP/2)\n","    instance = distance * p.stride/p.response_UP\n","    frame= instance * s_x/p.instance_size\n","    new_target_position = frame + target_position\n","\n","\n","\n","\n","    ##########-------END OF CODE-------##########\n","\n","    return new_target_position, best_scale\n","\n","\n","def run_tracker(img_list, target_position, target_size, save_to_file='./Tracking_Res.txt'):\n","\n","    # get the default parameters\n","    p = Config()\n","\n","    # load model\n","    net = torch.load('./SiamFCMModel.pth')\n","    net = net.to(device)\n","\n","    # evaluation mode\n","    net.eval()\n","\n","    # first frame\n","    img_uint8 = cv2.imread(img_list[0])\n","    img_uint8 = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2RGB)\n","    img_double = np.double(img_uint8)  # uint8 to float\n","\n","    # compute avg for padding\n","    avg_chans = np.mean(img_double, axis=(0, 1))\n","\n","\n","    # computing variables for later scaling and cropping operations\n","    # Try to understand these to get better understanding of how we pull target\n","    # image patch\n","    wc_z = target_size[1] + p.context_amount * sum(target_size)\n","    hc_z = target_size[0] + p.context_amount * sum(target_size)\n","    s_z = np.sqrt(wc_z * hc_z)\n","    scale_z = p.examplar_size / s_z\n","\n","    # crop examplar z in the first frame\n","    z_crop = get_subwindow_tracking(img_double, target_position, p.examplar_size, round(s_z), avg_chans)\n","    z_crop = np.uint8(z_crop)  # you need to convert it to uint8\n","    # convert image to tensor\n","    z_crop_tensor = 255.0 * F.to_tensor(z_crop).unsqueeze(0)\n","\n","\n","    # computing variables to pull appropriately sized \"area of interest\"s from search image\n","    d_search = (p.instance_size - p.examplar_size) / 2\n","    pad = d_search / scale_z\n","    s_x = s_z + 2 * pad\n","    # arbitrary scale saturation\n","    min_s_x = p.scale_min * s_x\n","    max_s_x = p.scale_max * s_x\n","\n","    # generate cosine window\n","    if p.windowing == 'cosine':\n","        window = np.outer(np.hanning(p.score_size * p.response_UP), np.hanning(p.score_size * p.response_UP))\n","    elif p.windowing == 'uniform':\n","        window = np.ones((p.score_size * p.response_UP, p.score_size * p.response_UP))\n","    window = window / sum(sum(window))\n","\n","    # pyramid scale search\n","    scales = p.scale_step ** np.linspace(-np.ceil(p.num_scale / 2), np.ceil(p.num_scale / 2), p.num_scale)\n","    # extract feature for examplar z\n","    z_features = net.feat_extraction(Variable(z_crop_tensor).to(device))\n","    z_features = z_features.repeat(p.num_scale, 1, 1, 1)\n","\n","    # do tracking\n","    bboxes = np.zeros((len(img_list), 4), dtype=np.double)  # save tracking result\n","    for i in tqdm(range(0, len(img_list)), desc=\"Tracking all frames in the sequence\"):\n","        # print('processing frame %d ...' %(i+1))\n","        if i > 0:\n","            # do detection\n","            # currently, we only consider RGB images for tracking\n","            img_uint8 = cv2.imread(img_list[i])\n","            img_uint8 = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2RGB)\n","            img_double = np.double(img_uint8)  # uint8 to float\n","\n","            scaled_instance = s_x * scales\n","            scaled_target = np.zeros((2, scales.size), dtype=np.double)\n","            scaled_target[0, :] = target_size[0] * scales\n","            scaled_target[1, :] = target_size[1] * scales\n","\n","            # extract scaled crops for search region x at previous target position\n","            x_crops = make_scale_pyramid(img_double, target_position, scaled_instance, p.instance_size, avg_chans, p)\n","\n","            # get features of search regions\n","            x_crops_tensor = torch.FloatTensor(x_crops.shape[3], x_crops.shape[2], x_crops.shape[1], x_crops.shape[0])\n","            # response_map = SiameseNet.get_response_map(z_features, x_crops)\n","            for k in range(x_crops.shape[3]):\n","                tmp_x_crop = x_crops[:, :, :, k]\n","                tmp_x_crop = np.uint8(tmp_x_crop)\n","                # numpy array to tensor\n","                x_crops_tensor[k, :, :, :] = 255.0 * F.to_tensor(tmp_x_crop).unsqueeze(0)\n","\n","            # get features of search regions\n","            # the input to the network is x_crop_tensors;\n","            # the feature of search regions is represented using 'x_features'\n","\n","            ##########--WRITE YOUR CODE HERE--##########\n","\n","\n","\n","\n","            x_features = net.feat_extraction(Variable(x_crops_tensor).to(device))\n","\n","\n","\n","\n","            ##########-------END OF CODE-------##########\n","\n","\n","            # evaluate the offline-trained network for exemplar x features\n","            target_position, new_scale = tracker_eval(net, round(s_x), z_features, x_features, target_position, window,\n","                                                      p)\n","\n","            # scale damping and saturation\n","            s_x = max(min_s_x, min(max_s_x, (1 - p.scale_LR) * s_x + p.scale_LR * scaled_instance[int(new_scale)]))\n","            target_size = (1 - p.scale_LR) * target_size + p.scale_LR * np.array(\n","                [scaled_target[0, int(new_scale)], scaled_target[1, int(new_scale)]])\n","\n","        rect_position = np.array(\n","            [target_position[1] - target_size[1] / 2, target_position[0] - target_size[0] / 2, target_size[1],\n","             target_size[0]])\n","\n","        # visualize the tracking results\n","        # comment for not visualizing\n","        if i % 50 == 0:\n","            visualize_tracking_result(img_uint8, rect_position, 1)\n","\n","        # output bbox in the original frame coordinates\n","        o_target_position = target_position\n","        o_target_size = target_size\n","        bboxes[i, :] = np.array(\n","            [o_target_position[1] - o_target_size[1] / 2, o_target_position[0] - o_target_size[0] / 2, o_target_size[1],\n","             o_target_size[0]])\n","    # save tracking results\n","    np.savetxt(save_to_file, bboxes, fmt='%.3f')\n","    return bboxes\n","\n","\n","if __name__ == \"__main__\":\n","  \n","    # load sequence\n","    img_list, target_position, target_size = load_sequence('./SiamFCVideo')\n","\n","    # run the tracker for given sequence and starting bounding box\n","    results = run_tracker(img_list, target_position, target_size, save_to_file='./Tracking_Result.txt')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lfy-YUJHZ3j_"},"source":["**After finishing the required parts and successfully running the code, please submit your code and the obtained tracking results in Tracking_Res.txt.**"]},{"cell_type":"markdown","metadata":{"id":"vgp7HCJxs8Bu"},"source":["# 2. RGB-T Tracking\n","\n","(15 points) In this problem, we will explore how information from visual spectrum and infrared spectrum can complement each other for tracking.\n","You are given a video sequence from the [RGB-T234 dataset](https://arxiv.org/abs/1805.08982): [RGB-T234/rainingwalking](https://drive.google.com/file/d/1LweAwT4hikySCIjtlsEi3BS0jfoU9iNn/view). \n","\n","It can be found in the directory:\n","```\n","project root\n","└───RGB-T234\n","│   │\n","│   └───rainingwalking\n","│       │   visible.txt\n","│       │   infrared.txt\n","│       └───visible\n","│       │      |  00123v.jpg\n","│       │      |  00124v.jpg\n","│       │      ...\n","|       |\n","│       └───infrared\n","│       │      |  00123i.jpg\n","│       │      |  00124i.jpg\n","│       │      ...\n","\n","```\n","\n","\n","In 2.1, lets run the tracker without using the thermal channel. Here we reuse above problem 1's implementation. I.e., after loading the above sequences we just pass the RGB image sequence and the bounding box info to run_tracker() method. Additionally, we find the 'success rate' of this tracking using available ground truth"]},{"cell_type":"markdown","metadata":{"id":"FIzTDi8SxOiq"},"source":["## 2.1 Perform tracking on a challenging video sequence and evaluate its performance\n","\n","\n","\n","(5 points) 1. Complete the load_sequence method to read the visible frames and infrared frames. Also, read the ground truth bounding box for each sequence from the \"Infrared.txt\" (just like you read the problem 1's groundtruth_rect.txt ).   \n","Note: We will be loading both RGB and Thermal video sequences. But in 2.1 we will only utilize the RGB image sequence.\n","\n","(0 point) 2. Once you complete the above method, you will be able to run the tracker on the extracted RGB video sequence. To evaluate how the tracker is perfroming we are going to compute \"Success Rate\" metric by comparing its results to ground truth boxes.  \n","  \"Success Rate\" is defined as:  \n","    ratio of successfully tracked frames in total frames. A frame is considered \"tracked\" if Interserction-Over-Union between predicted and actual bounding boxes is over a partivcular threshold (0.3 in our case)\n","  "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"In-hDMBwZrft","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AfgZHz74fsTJuGJtdLK-Z1F83lo-HXED"},"executionInfo":{"status":"ok","timestamp":1652281967429,"user_tz":240,"elapsed":85194,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"outputId":"84d38e6d-b89a-4024-809b-f2f5dfe335a2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def load_sequence(seq_path):\n","    \"\"\"\n","    load sequences;\n","    sequences should be in OTB format, or you can custom this function by yourself\n","\n","    NOTE: target_position, target_sz are derived from first bbox info from Infrared.txt\n","     \n","    \"\"\"\n","    # given the sequence path, you need to load sequence information, including\n","    # rgb_img_list: a list, each element of which is the path for an image from visible spectrum\n","    # t_img_list: a list, each element of which is the path for an image from thermal spectrum\n","    # target_position: an array, the center position of the target [center_y, center_x]\n","    # target_sz: an array, the height and width of the target [h, w]\n","    # gt: an array, of bounding box informations for each frame of the video sequence in format [(top_left_x, top_left_y, width, height), ...]\n","    # NOTE: the sequence folder contains a two sub-folders (visible and infrared) that contains all image files, \n","    # and a 'infrared.txt' that contains the initial groundtruth of the target in format (top_left_x, top_left_y, width, height)\n","\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","\n","    rgb_imgpath = os.path.join(seq_path,'visible')\n","    t_imgpath = os.path.join(seq_path,'infrared')\n","\n","    groundtruthfile = os.path.join(seq_path, 'infrared.txt')\n","\n","    rgb_img_list = []\n","    t_img_list = []\n","\n","    rgb_img_list = [imgfile for imgfile in glob.glob(os.path.join(rgb_imgpath, \"*.jpg\"))]\n","    t_img_list = [imgfile for imgfile in glob.glob(os.path.join(t_imgpath, \"*.jpg\"))]\n","\n","    rgb_img_list.sort()\n","    t_img_list.sort()\n","    \n","    groundtruth = open(groundtruthfile, 'r')\n","    reader = groundtruth.readline()\n","    region = [float(i) for i in reader.strip().split(\",\")]\n","    groundtruth.close()\n","    top_left_x, top_left_y = region[0], region[1]\n","    w, h = region[2], region[3]\n","    target_sz = np.array([h,w], dtype = np.double)\n","    cx = top_left_x + w/2\n","    cy = top_left_y + h/2\n","    target_position= np.array([cy, cx], dtype = np.double)\n","\n","    gt = []\n","    myfile = open(groundtruthfile, \"r\")\n","    myline = myfile.readline()\n","    while myline:\n","        items = myline.strip().split(\",\")\n","        gt.append(items)\n","        myline = myfile.readline()\n","    myfile.close()\n","    gt = np.array(gt, dtype = np.double)\n","\n","    ##########-------END OF CODE-------##########\n","    return rgb_img_list, t_img_list, target_position, target_sz, gt #CODE_CHANGE\n","\n","\n","def evaluate_success_rate(tracker_result, ground_truth, threshold=0.3):\n","    \"\"\"\n","    Success rate metric \n","    \"\"\"\n","    # tracker_result: an array of tracked object's bbox info\n","    # ground_truth: an array of tracked object's ground truth bbox info\n","    # threshold: a bbox is considered 'suscessfully tracking' if its iou w/ \n","    # groundtruth is higher than this threshold  \n","\n","    def iou(bbox1, bbox2):\n","        l1, l2 = bbox1[1], bbox2[1]\n","        r1, r2 = bbox1[1]+bbox1[3], bbox2[1]+bbox2[3]\n","        u1, u2 = bbox1[0], bbox2[0]\n","        b1, b2 = bbox1[0]+bbox1[2], bbox2[0]+bbox2[2]\n","        # compute the Intersection Rectangle's left, right, up, bottom positions \n","        l, r, u, b = max(l1, l2), min(r1, r2), max(u1, u2), min(b1, b2)\n","        h, w = max(0, b-u), max(0, r-l)\n","        # compute iou\n","        return h*w/( bbox1[3]*bbox1[2] + bbox2[3]*bbox2[2] - h*w )\n","\n","    total_frames = len(tracker_result)\n","    success_frames = 0\n","    for i in range(total_frames):\n","        _iou = iou(tracker_result[i], ground_truth[i])\n","        success_frames = success_frames + (_iou>threshold)\n","    return success_frames/total_frames\n","\n","if __name__ == \"__main__\":\n","  \n","    # load sequence\n","    img_list, _, target_position, target_size, gt = load_sequence('RGB-T234/rainingwaliking')\n","    img_list, _, target_position, target_size, gt = img_list, _, target_position, target_size, gt\n","    # run tracker\n","    results = run_tracker(img_list, target_position, target_size)\n","    # find average IOU of the tracking result\n","    print(\"Success Rate of the tracker on this sequence: \", evaluate_success_rate(results, gt))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NTUxpouUN2IR"},"source":["##2.2 Using Thermal images to improve the performance\n","\n","(10 points) In the above problem we have tried to track the man walking with an umberlla. \n","In this video [RGB-T234/rainingwalking](https://drive.google.com/file/d/1LweAwT4hikySCIjtlsEi3BS0jfoU9iNn/view), although we humans are able to see him and track him, the SiamFC tracker was not able to do that(given its success rate in above section is very less (<30%)). We could solve this by designing a model that is sensitive to even such inadequate visual cues. But this is a problem that needs carefull design.   \n","\n","So, another way to fix this is by gathering more information (rather than trying to understand limited data). We can capture the environment in infrared spectrum and perceive how different objects radiate heat. This extra data can be read as a new channel alongside RGB channels in each image and can potentially improve our tracking.\n","\n","\n","In the below code, we will use a second siamese extraction branch to extract features from thermal images and fuse them with the features from RGB images. This fusion helps encode both visual and thermal cues into the feature maps and thus lead to more accurate/sensitive response map.\n","\n","You can expect a success rate around 80% from this implementation.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_SQeCJV-s7C7","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1hCi-E760MwXLY5TYkQ8JHSbCfecpU0AM"},"executionInfo":{"status":"ok","timestamp":1652282528191,"user_tz":240,"elapsed":131882,"user":{"displayName":"amro halwah","userId":"09235503275673346987"}},"outputId":"7014a8ec-c132-41a1-ea1f-c988d067893c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def fuse_features(feat1_tensor, feat2_tensor):\n","    \"fuse the network features\"\n","    # feat1_tensor: feature of RGB channel image\n","    # feat2_tensor: feature of Thermal channel image\n","\n","    # since you use the same network on similarly shaped input images, \n","    # the features of RGB and T images will have same shape\n","    assert feat1_tensor.shape==feat2_tensor.shape\n","\n","    \n","    # You can experiment with multiple ways of feature fusion and \n","    # submit with whatever approch worked better\n","    ##########--WRITE YOUR CODE HERE--##########\n","\n","    return torch.cat((feat2_tensor,feat1_tensor), 0)\n","\n","\n","    ##########-------END OF CODE-------##########\n","\n","\n","if __name__ == \"__main__\":\n","\n","    # get the default parameters\n","    p = Config()\n","\n","    # load model\n","    net = torch.load('./SiamFCMModel.pth')\n","    net = net.to(device)\n","\n","    # evaluation mode\n","    net.eval()\n","\n","    # load sequences\n","    rgb_img_list, t_img_list, target_position, target_size, gt = load_sequence('./RGB-T234/rainingwaliking')\n","\n","    # first frame (Visual + Thermal)\n","    rgb_img_uint8 = cv2.imread(rgb_img_list[0])\n","    rgb_img_uint8 = cv2.cvtColor(rgb_img_uint8, cv2.COLOR_BGR2RGB)\n","    rgb_img_double = np.double(rgb_img_uint8)  # uint8 to float\n","\n","    t_img_uint8 = cv2.imread(t_img_list[0])\n","    t_img_double = np.double(t_img_uint8)\n","\n","    # compute avg for padding\n","    rgb_avg_chans = np.mean(rgb_img_double, axis=(0, 1))\n","    t_avg_chans = np.mean(t_img_double, axis=(0, 1))\n","\n","    wc_z = target_size[1] + p.context_amount * sum(target_size)\n","    hc_z = target_size[0] + p.context_amount * sum(target_size)\n","    s_z = np.sqrt(wc_z * hc_z)\n","    scale_z = p.examplar_size / s_z\n","\n","    # crop examplar z in the first frame\n","    rgb_z_crop = get_subwindow_tracking(rgb_img_double, target_position, p.examplar_size, round(s_z), rgb_avg_chans)\n","    rgb_z_crop = np.uint8(rgb_z_crop)  # you need to convert it to uint8\n","    # convert image to tensor\n","    rgb_z_crop_tensor = 255.0 * F.to_tensor(rgb_z_crop).unsqueeze(0)\n","    \n","    t_z_crop = get_subwindow_tracking(t_img_double, target_position, p.examplar_size, round(s_z), t_avg_chans)\n","    t_z_crop = np.uint8(t_z_crop)\n","    t_z_crop_tensor = 255.0 * F.to_tensor(t_z_crop).unsqueeze(0)\n","    \n","    # p.instance_size = 400\n","    d_search = (p.instance_size - p.examplar_size) / 2\n","    pad = d_search / scale_z\n","    s_x = s_z + 2 * pad\n","    # arbitrary scale saturation\n","    min_s_x = p.scale_min * s_x\n","    max_s_x = p.scale_max * s_x\n","\n","    # generate cosine window\n","    if p.windowing == 'cosine':\n","        window = np.outer(np.hanning(p.score_size * p.response_UP), np.hanning(p.score_size * p.response_UP))\n","    elif p.windowing == 'uniform':\n","        window = np.ones((p.score_size * p.response_UP, p.score_size * p.response_UP))\n","    window = window / sum(sum(window))\n","\n","    # pyramid scale search\n","    scales = p.scale_step ** np.linspace(-np.ceil(p.num_scale / 2), np.ceil(p.num_scale / 2), p.num_scale)\n","    # extract feature for examplar z\n","    rgb_z_features = net.feat_extraction(Variable(rgb_z_crop_tensor).to(device))\n","    t_z_features = net.feat_extraction(Variable(t_z_crop_tensor).to(device))\n","    z_features = fuse_features(rgb_z_features, t_z_features)\n","    z_features = z_features.repeat(p.num_scale, 1, 1, 1)\n","\n","    # do tracking\n","    bboxes = np.zeros((len(rgb_img_list), 4), dtype=np.double)  # save tracking result\n","    for i in tqdm(range(0, len(rgb_img_list)), desc=\"Tracking all frames in the sequence\"):\n","        # print('processing frame %d (%s) ...' %(i+1, rgb_img_list[i]))\n","        if i > 0:\n","            # do detection\n","            # currently, we only consider RGB images for tracking\n","            rgb_img_uint8 = cv2.imread(rgb_img_list[i])\n","            t_img_uint8 = cv2.imread(t_img_list[i])\n","            rgb_img_uint8 = cv2.cvtColor(rgb_img_uint8, cv2.COLOR_BGR2RGB)\n","            rgb_img_double = np.double(rgb_img_uint8)  # uint8 to float\n","            t_img_double = np.double(t_img_uint8)\n","\n","            scaled_instance = s_x * scales\n","            scaled_target = np.zeros((2, scales.size), dtype=np.double)\n","            scaled_target[0, :] = target_size[0] * scales\n","            scaled_target[1, :] = target_size[1] * scales\n","\n","            # extract scaled crops for search region x at previous target position\n","            rgb_x_crops = make_scale_pyramid(rgb_img_double, target_position, scaled_instance, p.instance_size, rgb_avg_chans, p)\n","            # repeat the above same for current thermal frame\n","            t_x_crops = make_scale_pyramid(t_img_double, target_position, scaled_instance, p.instance_size, t_avg_chans, p)\n","\n","            # get features of search regions\n","            rgb_x_crops_tensor = torch.FloatTensor(rgb_x_crops.shape[3], rgb_x_crops.shape[2], rgb_x_crops.shape[1], rgb_x_crops.shape[0])\n","            t_x_crops_tensor = torch.FloatTensor(rgb_x_crops.shape[3], rgb_x_crops.shape[2], rgb_x_crops.shape[1], rgb_x_crops.shape[0])\n","\n","            for k in range(rgb_x_crops.shape[3]):\n","                tmp_x_crop = np.uint8(rgb_x_crops[:, :, :, k])\n","                # numpy array to tensor\n","                rgb_x_crops_tensor[k, :, :, :] = 255.0 * F.to_tensor(tmp_x_crop).unsqueeze(0)\n","                # repeat for t frame\n","                tmp_x_crop = np.uint8(t_x_crops[:, :, :, k]) \n","                t_x_crops_tensor[k, :, :, :] = 255.0 * F.to_tensor(tmp_x_crop).unsqueeze(0)\n","\n","\n","            # get features of search regions;\n","            # the input to the network branches are rgb_x_crops_tensor and t_x_crops_tensor;\n","            # the feature of search regions is represented using 'x_features'\n","            # use the fuse_features() method that you have implemented to fuse the features output by network branches\n","            # NOTE: For simplicity you can use the same siamese network to extract features of thrermal channels \n","            ##########--WRITE YOUR CODE HERE--##########\n","\n","\n","\n","            x_crops_tensor = fuse_features(rgb_x_crops_tensor, t_x_crops_tensor)\n","            x_features = net.feat_extraction(Variable(x_crops_tensor).to(device))\n","\n","\n","\n","\n","\n","            ##########-------END OF CODE-------##########\n","\n","            # evaluate the offline-trained network for exemplar x features\n","            target_position, new_scale = tracker_eval(net, round(s_x), z_features, x_features, target_position, window,\n","                                                      p)\n","            # scale damping and saturation\n","            s_x = max(min_s_x, min(max_s_x, (1 - p.scale_LR) * s_x + p.scale_LR * scaled_instance[int(new_scale)]))\n","            target_size = (1 - p.scale_LR) * target_size + p.scale_LR * np.array(\n","                [scaled_target[0, int(new_scale)], scaled_target[1, int(new_scale)]])\n","\n","        rect_position = np.array(\n","            [target_position[1] - target_size[1] / 2, target_position[0] - target_size[0] / 2, target_size[1],\n","             target_size[0]])\n","\n","        # visualize the tracking results\n","        # comment for not visualizing\n","        if i % 50 == 0:\n","            visualize_tracking_result(rgb_img_uint8, rect_position, 1)\n","\n","        # output bbox in the original frame coordinates\n","        o_target_position = target_position\n","        o_target_size = target_size\n","        bboxes[i, :] = np.array(\n","            [o_target_position[1] - o_target_size[1] / 2, o_target_position[0] - o_target_size[0] / 2, o_target_size[1],\n","             o_target_size[0]])\n","    # save tracking results\n","    np.savetxt('./Tracking_Res_RGBT.txt', bboxes, fmt='%.3f')\n","    print('\\n')\n","    print(\"Success Rate of the tracker on this sequence: \", evaluate_success_rate(bboxes, gt))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PKkeZT1ICkt9"},"source":["## Submission guidelines\n","---\n","Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_hw5' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs. **DO NOT** zip and upload the dataset on blackboard due to size limit.\n","\n","When submitting your .zip file through blackboard, please\n","-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n","\n","This zip file should include:\n","```\n","Surname_Givenname_SBUID_hw*\n","        |---Surname_Givenname_SBUID_hw*.ipynb\n","        |---Surname_Givenname_SBUID_hw*.pdf\n","```\n","\n","For instance, student Yann LeCun should submit a zip file named \"LeCun_Yann_111134567_hw5.zip\" for homework5 in this structure:\n","```\n","LeCun_Yann_111134567_hw5\n","        |---LeCun_Yann_111134567_hw5_p1.ipynb\n","        |---LeCun_Yann_111134567_hw5_p2.ipynb\n","        |---LeCun_Yann_111134567_hw5.pdf\n","```\n","\n","The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link**. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n","\n","Then right click this folder, click ***Get shareable link***, in the People textfield, enter TA's email: ***kgarigapati@cs.stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n","\n","Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n","\n","**Late submission penalty:** <br>\n","There will be a 10% penalty per day for late submission. However, you will have 3 days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CSE527-22S-HW5-P1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}